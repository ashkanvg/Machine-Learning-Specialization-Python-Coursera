1- Classification 
2- Binary Classification
	- 0/1	False/True
3- Classification with Logistic Regression 
	- logistic regresiion
	- sigmoid function
		+ g(z) = 1/(1+e^(-z)) 		0<g(z)<1   
		+ z = w.x+b
4- Decision Boundary
	- w.x+b = 0
	- w.x+b>0 --> y=1
	- w.x+b<0 --> y=0

5- Cost Function for Logistic Regression
	- mean square is not good for logistic regression --> beacuse made it non-convex
	- Loss function:
		+ if y(i)=1 --> L( f(x(i)),y(i) ) = -log( f(x(i) )
		+ if y(i)=0 --> L( f(x(i)),y(i) ) = -log( 1 - f(x(i) )
	- simple loss function:
		+ L( f(x(i)),y(i) ) = -y*log( f(x(i) ) - (1-y)*log( 1 - f(x(i) )
	- Cost:
		+ J(w,b) = sigma( L( f(x(i)),y(i) ) ) * (1/m)

6- Gradient Descent for Logistic Regression
	- same as before
	- finding learning rate is still our challenge
	- don't forget feature scaling
	- vectorization is still important

7- Overfitting(high variance) Vs. Underfitting(high bias)
	- low features --> overfit
	- many features --> overfit

8- Addressing Overfitting
	- collect more training data
		+ problem: have few datas
	- use feautre features --> use small subset of features
		+ disadvantages: throwing away some of features
	- regularization

8- regularization
	- way to reduce impact of some features by eliminate it
	- regularization term: (lambda/2m)*( sigma(w(j)^2) ) 	, lambda>0
		+ if lambda = 0 --> no regularization
		+ if lambda = very very large --> w(j) near to 0 --> f(x)=b
	- Regularization for Cost Function (linear regression and logistic regression):
		+ J(w,b) = J(w,b) + (lambda/2m)*( sigma(w(j)^2) ) + (lambda/2m)*( b^2 ) 
		+ J(w,b) = J(w,b) + (lambda/2m)*( sigma(w(j)^2) ) [we used]
	- Regularization for Linear Regression 
		+ dj_dw = (1/m)(sigma[ (f(x(i))-y(i))*x(i,j) ]+(lambda/m)*(w(j))
	- Regularization for Logistic Regression 
		+ dj_dw = (1/m)(sigma[ (f(x(i))-y(i))*x(i,j) ]+(lambda/m)*(w(j))
	- Reguularization for gradient descent
		+ wj = wj(1-(alpha/lambda)) - [same as before]     j=1,...,m